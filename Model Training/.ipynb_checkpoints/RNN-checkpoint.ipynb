{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voV4m0uxDxHs"
   },
   "source": [
    "# Recurrent Neural Network (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LpDZRgNc5kNV"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uF7m4yzQ6OtT"
   },
   "outputs": [],
   "source": [
    "#Taking the EDA data \n",
    "with open('EDA.pickle', 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "N8NNkqpJ7ypT",
    "outputId": "4b98acb7-0749-4c6e-b00a-8951f73e08ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>mean_d_0_a2</th>\n",
       "      <th>mean_d_1_a2</th>\n",
       "      <th>mean_d_2_a2</th>\n",
       "      <th>mean_d_3_a2</th>\n",
       "      <th>mean_d_4_a2</th>\n",
       "      <th>mean_d_5_a</th>\n",
       "      <th>mean_d_6_a</th>\n",
       "      <th>mean_d_7_a</th>\n",
       "      <th>mean_d_8_a</th>\n",
       "      <th>mean_d_9_a</th>\n",
       "      <th>mean_d_10_a</th>\n",
       "      <th>mean_d_11_a</th>\n",
       "      <th>mean_d_12_a</th>\n",
       "      <th>mean_d_13_a</th>\n",
       "      <th>mean_d_14_a</th>\n",
       "      <th>mean_d_15_a</th>\n",
       "      <th>mean_d_16_a</th>\n",
       "      <th>mean_d_17_a</th>\n",
       "      <th>mean_d_18_a</th>\n",
       "      <th>mean_d_19_a</th>\n",
       "      <th>mean_d_20_a</th>\n",
       "      <th>mean_d_21_a</th>\n",
       "      <th>mean_d_22_a</th>\n",
       "      <th>mean_d_23_a</th>\n",
       "      <th>mean_d_24_a</th>\n",
       "      <th>mean_d_25_a</th>\n",
       "      <th>mean_d_26_a</th>\n",
       "      <th>mean_d_27_a</th>\n",
       "      <th>mean_d_28_a</th>\n",
       "      <th>mean_d_29_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_711_b</th>\n",
       "      <th>fft_712_b</th>\n",
       "      <th>fft_713_b</th>\n",
       "      <th>fft_714_b</th>\n",
       "      <th>fft_715_b</th>\n",
       "      <th>fft_716_b</th>\n",
       "      <th>fft_717_b</th>\n",
       "      <th>fft_718_b</th>\n",
       "      <th>fft_719_b</th>\n",
       "      <th>fft_720_b</th>\n",
       "      <th>fft_721_b</th>\n",
       "      <th>fft_722_b</th>\n",
       "      <th>fft_723_b</th>\n",
       "      <th>fft_724_b</th>\n",
       "      <th>fft_725_b</th>\n",
       "      <th>fft_726_b</th>\n",
       "      <th>fft_727_b</th>\n",
       "      <th>fft_728_b</th>\n",
       "      <th>fft_729_b</th>\n",
       "      <th>fft_730_b</th>\n",
       "      <th>fft_731_b</th>\n",
       "      <th>fft_732_b</th>\n",
       "      <th>fft_733_b</th>\n",
       "      <th>fft_734_b</th>\n",
       "      <th>fft_735_b</th>\n",
       "      <th>fft_736_b</th>\n",
       "      <th>fft_737_b</th>\n",
       "      <th>fft_738_b</th>\n",
       "      <th>fft_739_b</th>\n",
       "      <th>fft_740_b</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2.15</td>\n",
       "      <td>29.5</td>\n",
       "      <td>-353.0</td>\n",
       "      <td>14.40</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5.98</td>\n",
       "      <td>30.7</td>\n",
       "      <td>-343.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>27.9</td>\n",
       "      <td>3.17</td>\n",
       "      <td>32.2</td>\n",
       "      <td>-368.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>36.4</td>\n",
       "      <td>7.08</td>\n",
       "      <td>28.8</td>\n",
       "      <td>-359.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>19.6</td>\n",
       "      <td>-3.8300</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>-10.80000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-6.41</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-2.780</td>\n",
       "      <td>14.60</td>\n",
       "      <td>-1.540</td>\n",
       "      <td>-14.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.90</td>\n",
       "      <td>-24.4</td>\n",
       "      <td>-24.4</td>\n",
       "      <td>30.90</td>\n",
       "      <td>-442.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>-564.0</td>\n",
       "      <td>500.00</td>\n",
       "      <td>-245.00</td>\n",
       "      <td>-245.00</td>\n",
       "      <td>500.00</td>\n",
       "      <td>-88.8</td>\n",
       "      <td>214.0</td>\n",
       "      <td>-88.8</td>\n",
       "      <td>-88.8</td>\n",
       "      <td>214.0</td>\n",
       "      <td>-606.0</td>\n",
       "      <td>509.000</td>\n",
       "      <td>-261.0</td>\n",
       "      <td>-261.0</td>\n",
       "      <td>509.000</td>\n",
       "      <td>-399.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>-185.00</td>\n",
       "      <td>-185.00</td>\n",
       "      <td>374.0</td>\n",
       "      <td>74.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>25.60</td>\n",
       "      <td>32.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>21.50</td>\n",
       "      <td>17.4</td>\n",
       "      <td>25.50</td>\n",
       "      <td>31.7</td>\n",
       "      <td>31.5</td>\n",
       "      <td>26.2</td>\n",
       "      <td>32.9</td>\n",
       "      <td>31.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>33.2</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.8</td>\n",
       "      <td>32.40</td>\n",
       "      <td>34.7</td>\n",
       "      <td>33.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-1.87000</td>\n",
       "      <td>-4.690</td>\n",
       "      <td>-15.40</td>\n",
       "      <td>-6.22</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-6.980</td>\n",
       "      <td>-9.370000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.99</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>-5.99</td>\n",
       "      <td>163.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>17.90</td>\n",
       "      <td>17.90</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>25.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>225.0</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-27.7</td>\n",
       "      <td>-27.7</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>97.4</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>40.70</td>\n",
       "      <td>40.70</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>7.75</td>\n",
       "      <td>30.1</td>\n",
       "      <td>-441.0</td>\n",
       "      <td>9.89</td>\n",
       "      <td>25.3</td>\n",
       "      <td>-68.90</td>\n",
       "      <td>25.3</td>\n",
       "      <td>-481.0</td>\n",
       "      <td>-65.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>79.80</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-408.0</td>\n",
       "      <td>91.9</td>\n",
       "      <td>29.5</td>\n",
       "      <td>18.80</td>\n",
       "      <td>31.1</td>\n",
       "      <td>-335.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>19.9</td>\n",
       "      <td>76.6000</td>\n",
       "      <td>4.850</td>\n",
       "      <td>39.90000</td>\n",
       "      <td>75.300</td>\n",
       "      <td>5.27</td>\n",
       "      <td>-72.00</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-33.00</td>\n",
       "      <td>-82.000</td>\n",
       "      <td>-4.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>585.00</td>\n",
       "      <td>-285.0</td>\n",
       "      <td>-285.0</td>\n",
       "      <td>585.00</td>\n",
       "      <td>-94.3</td>\n",
       "      <td>183.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>-133.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>-72.00</td>\n",
       "      <td>-72.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>-158.0</td>\n",
       "      <td>-158.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>-251.0</td>\n",
       "      <td>255.000</td>\n",
       "      <td>-95.7</td>\n",
       "      <td>-95.7</td>\n",
       "      <td>255.000</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>-48.80</td>\n",
       "      <td>-48.80</td>\n",
       "      <td>159.0</td>\n",
       "      <td>-534.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>17.30</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>20.40</td>\n",
       "      <td>22.8</td>\n",
       "      <td>13.20</td>\n",
       "      <td>31.5</td>\n",
       "      <td>-147.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>27.7</td>\n",
       "      <td>15.70</td>\n",
       "      <td>30.7</td>\n",
       "      <td>-142.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>22.8</td>\n",
       "      <td>13.60</td>\n",
       "      <td>32.2</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>23.8</td>\n",
       "      <td>4.1500</td>\n",
       "      <td>0.556</td>\n",
       "      <td>-0.35400</td>\n",
       "      <td>3.460</td>\n",
       "      <td>-4.96</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.330</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>...</td>\n",
       "      <td>249.00</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>249.00</td>\n",
       "      <td>359.0</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>-7.17</td>\n",
       "      <td>-7.17</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>-296.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-218.0</td>\n",
       "      <td>-218.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>-113.000</td>\n",
       "      <td>38.4</td>\n",
       "      <td>38.4</td>\n",
       "      <td>-113.000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>-61.9</td>\n",
       "      <td>-5.08</td>\n",
       "      <td>-5.08</td>\n",
       "      <td>-61.9</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>26.10</td>\n",
       "      <td>34.3</td>\n",
       "      <td>43.7</td>\n",
       "      <td>23.70</td>\n",
       "      <td>20.6</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>34.1</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>59.40</td>\n",
       "      <td>26.7</td>\n",
       "      <td>60.3</td>\n",
       "      <td>64.7</td>\n",
       "      <td>26.9</td>\n",
       "      <td>32.30</td>\n",
       "      <td>30.1</td>\n",
       "      <td>33.6</td>\n",
       "      <td>31.6</td>\n",
       "      <td>27.7</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.00677</td>\n",
       "      <td>33.700</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>-33.30</td>\n",
       "      <td>7.640</td>\n",
       "      <td>-16.60</td>\n",
       "      <td>-41.100</td>\n",
       "      <td>-6.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.30</td>\n",
       "      <td>-11.9</td>\n",
       "      <td>-11.9</td>\n",
       "      <td>18.30</td>\n",
       "      <td>159.0</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>-25.6</td>\n",
       "      <td>-25.6</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>194.0</td>\n",
       "      <td>32.70</td>\n",
       "      <td>-54.50</td>\n",
       "      <td>-54.50</td>\n",
       "      <td>32.70</td>\n",
       "      <td>154.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>-56.4</td>\n",
       "      <td>-56.4</td>\n",
       "      <td>26.4</td>\n",
       "      <td>252.0</td>\n",
       "      <td>9.940</td>\n",
       "      <td>-37.4</td>\n",
       "      <td>-37.4</td>\n",
       "      <td>9.940</td>\n",
       "      <td>172.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>-7.25</td>\n",
       "      <td>-7.25</td>\n",
       "      <td>15.9</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  ...  fft_748_b  fft_749_b     label\n",
       "0        4.62      30.3    -356.0  ...    -162.00     280.00  NEGATIVE\n",
       "1       28.80      33.1      32.0  ...     -31.60       2.57   NEUTRAL\n",
       "2        8.90      29.4    -416.0  ...    -148.00     281.00  POSITIVE\n",
       "3       14.90      31.6    -143.0  ...       9.53     -12.40  POSITIVE\n",
       "4       28.30      31.3      45.2  ...      23.90     -17.60   NEUTRAL\n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial values of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-D9cqOhB70JI",
    "outputId": "d0672c73-921b-464d-ea59-459d1410658b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2549)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7eGI_Ie771dy"
   },
   "outputs": [],
   "source": [
    "#Lable mapping\n",
    "label_mapping = {'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iyjMUN9b719V"
   },
   "outputs": [],
   "source": [
    "#Function for creating train test split\n",
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['label'] = df['label'].replace(label_mapping)\n",
    "    \n",
    "    y = df['label'].copy()\n",
    "    X = df.drop('label', axis=1).copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "-JtAKAER73dd"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZP83JoU75oI",
    "outputId": "f3c18e86-8991-462c-fda2-4bc2567125da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (1492, 2548)\n",
      "Shape of Testing Dataset: (640, 2548)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Training Dataset:',X_train.shape)\n",
    "print('Shape of Testing Dataset:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLD9GCKf764m",
    "outputId": "ab913343-cd63-45cf-b79c-6266ea3def24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 2548)]            0         \n",
      "_________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)  (None, 2548, 1)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 2548, 256)         198912    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 652288)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 1956867   \n",
      "=================================================================\n",
      "Total params: 2,155,779\n",
      "Trainable params: 2,155,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Modeling\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "expand_dims = tf.expand_dims(inputs, axis=2)\n",
    "\n",
    "gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(gru)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "p9Esyp0J9hiz"
   },
   "outputs": [],
   "source": [
    "#Model Compiling\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caJ23End9z6n",
    "outputId": "8e27efe4-81ef-46b8-b1f0-a84bcfe50de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - 270s 5s/step - loss: 55.6676 - accuracy: 0.6984 - val_loss: 9.5306 - val_accuracy: 0.9152\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 292s 6s/step - loss: 4.8071 - accuracy: 0.9238 - val_loss: 10.4325 - val_accuracy: 0.8326\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 290s 5s/step - loss: 4.8624 - accuracy: 0.9254 - val_loss: 13.6230 - val_accuracy: 0.9040\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 287s 5s/step - loss: 2.1916 - accuracy: 0.9583 - val_loss: 10.8025 - val_accuracy: 0.9129\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 296s 6s/step - loss: 2.9504 - accuracy: 0.9554 - val_loss: 3.0084 - val_accuracy: 0.9464\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 291s 5s/step - loss: 0.7453 - accuracy: 0.9847 - val_loss: 3.7999 - val_accuracy: 0.9509\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 285s 5s/step - loss: 0.2025 - accuracy: 0.9948 - val_loss: 6.8700 - val_accuracy: 0.9531\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 283s 5s/step - loss: 1.0713 - accuracy: 0.9847 - val_loss: 10.5972 - val_accuracy: 0.9263\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 282s 5s/step - loss: 3.0610 - accuracy: 0.9672 - val_loss: 6.6584 - val_accuracy: 0.9487\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 283s 5s/step - loss: 1.7070 - accuracy: 0.9778 - val_loss: 7.6491 - val_accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    batch_size=20,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "eZD7FQIv-7mW"
   },
   "outputs": [],
   "source": [
    "#Prediction \n",
    "rnn_pred = np.array(list(map(lambda x: np.argmax(x), model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OapoxUo4-UUC",
    "outputId": "5a801f7f-f16a-45a9-db5b-b8b79e6f1ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 97.855%\n"
     ]
    }
   ],
   "source": [
    "#Train Set Accuracy\n",
    "model_train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "print(\"Training Accuracy: {:.3f}%\".format(model_train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPfqm28Q97mx",
    "outputId": "b116e615-2a36-4db1-ddf5-0887bd0bca51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.312%\n"
     ]
    }
   ],
   "source": [
    "#Test Set Accuracy\n",
    "model_test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Test Accuracy: {:.3f}%\".format(model_test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBQ4R-ke-TMV",
    "outputId": "cb96dc0e-faf4-4460-cf55-c6af040cf15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.92      0.97      0.94       201\n",
      "     NEUTRAL       1.00      0.98      0.99       231\n",
      "    POSITIVE       0.94      0.91      0.93       208\n",
      "\n",
      "    accuracy                           0.95       640\n",
      "   macro avg       0.95      0.95      0.95       640\n",
      "weighted avg       0.95      0.95      0.95       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(y_test, rnn_pred, target_names=label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "TTjZbiAq-1TX",
    "outputId": "3d816207-fcfe-4561-a2e7-3004be54c2b5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "cm = confusion_matrix(y_test, rnn_pred)\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.yticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"ConfusionMatrix_rnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "a3T_cd3zAWtT"
   },
   "outputs": [],
   "source": [
    "#Creating dictionary for storing the accuracy details\n",
    "d = {\n",
    "     'Model': 'Recurrent Neural Network',\n",
    "     'Training Set Accuracy': model_train_acc,\n",
    "     'Test Set Accuracy':model_test_acc\n",
    "}\n",
    "\n",
    "#Creating Data Frame \n",
    "df_models_rnn = pd.DataFrame(d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "uRaV1vWtAv1F",
    "outputId": "9371281b-c76f-4d65-8f1a-517ee3120425"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recurrent Neural Network</td>\n",
       "      <td>0.978552</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Training Set Accuracy  Test Set Accuracy\n",
       "0  Recurrent Neural Network               0.978552           0.953125"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "l5Z2TWBUBJeE"
   },
   "outputs": [],
   "source": [
    "#Creating pickle files for further use\n",
    "model.save('../Models/best_rnn.h5')\n",
    "    \n",
    "with open('../Models/df_models_rnn.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_nn, output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
